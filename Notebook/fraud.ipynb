{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from ipywidgets import interact\n",
    "from sklearn.impute import KNNImputer\n",
    "import tensorflow as tf\n",
    "from scipy import stats\n",
    "import math\n",
    "from math import sqrt, pi\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.cluster import DBSCAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(r'C:\\Users\\engmo\\OneDrive\\Desktop\\python_trials\\bank_transactions_data_2.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting to know the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Channel unique values: {df[\"Channel\"].unique()}')\n",
    "print(f'Account ID unique values: {len(df[\"AccountID\"].unique())}')\n",
    "print(f'Transaction Type values: {df[\"TransactionType\"].unique()}')\n",
    "print(f'Merchant Id Unique values: {len(df[\"MerchantID\"].unique())}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"TransactionDate\"] = pd.to_datetime(df[\"TransactionDate\"])\n",
    "df[\"PreviousTransactionDate\"] = pd.to_datetime(df[\"PreviousTransactionDate\"])\n",
    "df[\"DaysBetweenTransaction\"] = (df[\"PreviousTransactionDate\"] - df[\"TransactionDate\"]).dt.days\n",
    "df[\"DaysBetweenTransaction\"] = df[\"DaysBetweenTransaction\"].astype(\"float64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = [\"TransactionType\", \"Channel\",\"CustomerOccupation\"]# to be used in model\n",
    "other_cols =  [col for col in df.columns if df[col].dtype in [\"float64\", \"int64\"]]\n",
    "\n",
    "cols = categorical_cols + other_cols\n",
    "x = df[cols].copy()\n",
    "l= df[cols].copy()# another copy that won't be transformed\n",
    "x = pd.get_dummies(x, columns=categorical_cols)\n",
    "l=pd.get_dummies(l,columns=categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x['percentage of transaction from account']=x['TransactionAmount']/x['AccountBalance']\n",
    "l['percentage of transaction from account']=l['TransactionAmount']/l['AccountBalance'] # is the dataframe that won't be transformed and would be used in comparison later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# univariate Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in x.select_dtypes(include=\"number\").columns:\n",
    "  sns.boxplot(data=x, x=i)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in x.select_dtypes(include=\"number\").columns:#loop about the column not a range\n",
    "  #sns.histplot(data[i])\n",
    "  plt.figure(figsize=(15, 6))  # Optional: Set figure size for better visibility\n",
    "  sns.histplot(x[i], stat=\"count\")\n",
    "\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multivariate Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = x.corr()\n",
    "\n",
    "plt.figure(figsize=(100, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', square=True, cbar_kws={\"shrink\": .8})\n",
    "plt.title('Heat Map of Correlation Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''''\n",
    "correlation_matrix = y.corr()\n",
    "\n",
    "plt.figure(figsize=(100, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm', square=True, cbar_kws={\"shrink\": .8})\n",
    "plt.title('Heat Map of Correlation Matrix')\n",
    "plt.show()\n",
    "'''''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## important notes\n",
    "1-as age increases account balance increases but transactions amount decreases\n",
    "\n",
    "2-credit is more used online or in branch and depit is more used from ATM\n",
    "\n",
    "3-more log in attempts increases slighltly wiht duration\n",
    "\n",
    "4-Despite the fact that students have lower account balances yet they make more amount of money in transactions than other occupations\n",
    "\n",
    "5-Doctors have the highest accounts yet the least spending behaviour\n",
    "\n",
    "6-except for the age and the transaction balance correlation which is 0.32 and the correlations between channels and types of payment , slighltly weak correlations are found among other features \n",
    "so clustering is needed to find complex patterns based on related features and to try to detect anomalies per cluster for possible frauds "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset prepation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## transforming features chosen in x data frame  (not the one hot encoded features) to normal distirbution using scipy fixing skewnees and checking using log like hood and shapiro test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''''\n",
    "def log_likelihood(data, lambda_value):\n",
    "    n = len(data)\n",
    "    mean = np.mean(data)\n",
    "    variance = np.var(data)\n",
    "    log_likelihood_value= -0.5 * (np.log(2 * np.pi * variance) + (1 / variance) * np.sum((data - mean) ** 2))*n\n",
    "    return log_likelihood_value\n",
    "\n",
    "features_to_transform=[\n",
    "          'TransactionAmount',\n",
    "          'DaysBetweenTransaction',\n",
    "          'AccountBalance',\n",
    "          'TransactionDuration',\n",
    "          'percentage of transaction from account',\n",
    "          'CustomerAge'\n",
    "]\n",
    "# Loop through each feature\n",
    "for feature in features_to_transform:\n",
    "    # Check for non-positive values\n",
    "    if (x[feature] <= 0).any():\n",
    "        # Option 1: Remove non-positive values\n",
    "        x = x[x[feature] > 0]\n",
    "    \n",
    "    # Reset index after filtering\n",
    "    x.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "print(x)\n",
    "\n",
    "\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "# Convert to NumPy array and apply Box-Cox transformation\n",
    "for i in features_to_transform:\n",
    "    original=x[i]\n",
    "    transformed_values, lambda_value = stats.boxcox(x[i].to_numpy())\n",
    "\n",
    "# Add transformed values back to the DataFrame\n",
    "    x[i] = transformed_values #updated\n",
    "\n",
    "    print(x[i])\n",
    "    plt.figure(figsize=(15, 6))  # Optional: Set figure size for better visibility\n",
    "    sns.histplot(x[i], stat=\"count\")\n",
    "\n",
    "    # Calculate log-likelihood for transformed data\n",
    "    print(f'Optimal lambda for {i} data is: {lambda_value}')\n",
    "    ll_value = log_likelihood(x[i].dropna().to_numpy(), lambda_value)\n",
    "    print(f'Log-Likelihood of transformed {i} is: {ll_value}')\n",
    "    plt.figure(figsize=(15, 6))\n",
    "\n",
    "    # Original Data Histogram\n",
    "    plt.subplot(1, 2, 1)\n",
    "    sns.histplot(original, kde=True, stat=\"count\")\n",
    "    plt.title(f'Original  Data {i} Distribution')\n",
    "    plt.xlabel(f'Values of {i}')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    # Transformed Data Histogram\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.histplot(x[i], kde=True, stat=\"count\")  # Using the updated Diphtheria column\n",
    "    plt.title(f'Transformed {i} Data Distribution (Box-Cox)')\n",
    "    plt.xlabel(f'Transformed Values of {i}')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Shapiro-Wilk test for transformed data\n",
    "    shapiro_test = stats.shapiro(x[i].dropna())\n",
    "    print(f'Shapiro-Wilk Test: Statistic={shapiro_test.statistic}, p-value={shapiro_test.pvalue}')\n",
    "\n",
    "    # Q-Q plot for transformed data\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    stats.probplot(x[i].dropna(), dist=\"norm\", plot=plt)\n",
    "    plt.title(f'Q-Q Plot of Transformed {i} Data')\n",
    "    plt.show()\n",
    "\n",
    "'''''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## Standarizning Data in both data frames \n",
    "\n",
    "\n",
    "# Assuming x is your DataFrame\n",
    "# Define features as all column names of the DataFrame\n",
    "features = x.columns.tolist()  # Get all column names as a list\n",
    "\n",
    "\n",
    "for i in features:\n",
    "    # Original values\n",
    "    original = x[i].dropna()  \n",
    "    original_2 = l[i].dropna() # Drop NaN values for accurate calculations\n",
    "\n",
    "    # Calculate mean and standard deviation\n",
    "    mean = original.mean()\n",
    "    mean2= original_2.mean()\n",
    "    std_dev = original.std()\n",
    "    std_dev2 = original_2.std()\n",
    "\n",
    "    # Apply Z-score normalization\n",
    "    x[i] = (original - mean) / std_dev\n",
    "    l[i]=(original_2-mean2)/std_dev2\n",
    "\n",
    "    # Print transformed values\n",
    "    print(f\" standatized  Transformed values for {i}:\\n{x[i]}\")\n",
    "    print(f\"standaruzed original values for {i}:\\n{l[i]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# clustering data into groups based on specefic features to try find complex patterns and remove farthest points in each cluster\n",
    "Features to cluster on are\n",
    "\n",
    "**transaction type**\n",
    "\n",
    "**percentage of transaction from account total**\n",
    "\n",
    "**transaction channel**\n",
    "\n",
    "**TransactionAmount**\n",
    "\n",
    "**DaysBetweenTransaction**\n",
    "\n",
    "**AccountBalance**\n",
    "\n",
    "**TransactionDuration**\n",
    "\n",
    "**CustomerAge**\n",
    "\n",
    "**LoginAttempts**\n",
    "\n",
    "**Customer_occupation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K__means clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "\n",
    "# Range of number of clusters for Elbow Method\n",
    "range_n_clusters = range(2, 11)\n",
    "inertia = []\n",
    "\n",
    "# Calculate inertia for each number of clusters\n",
    "for n_clusters in range_n_clusters:\n",
    "    clusterer = KMeans(n_clusters=n_clusters, init=\"k-means++\", n_init='auto', random_state=1)\n",
    "    cluster_labels = clusterer.fit_predict(x)\n",
    "    \n",
    "    # Store the inertia\n",
    "    inertia.append(clusterer.inertia_)\n",
    "\n",
    "# Plotting the Elbow Method\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range_n_clusters, inertia, marker='o')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Inertia (Sum of Squared Distances)')\n",
    "plt.xticks(range_n_clusters)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Silhouette Analysis (your existing code)\n",
    "for n_clusters in range_n_clusters:\n",
    "    fig, ax1 = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "    ax1.set_xlim([-0.1, 1])\n",
    "    ax1.set_ylim([0, len(x) + (n_clusters + 1) * 10])\n",
    "\n",
    "    clusterer = KMeans(n_clusters=n_clusters, init=\"k-means++\", n_init='auto', random_state=1)\n",
    "    cluster_labels = clusterer.fit_predict(x)\n",
    "\n",
    "    silhouette_avg = silhouette_score(x, cluster_labels)\n",
    "    print(\n",
    "        \"For n_clusters =\",\n",
    "        n_clusters,\n",
    "        \"The average silhouette_score is :\",\n",
    "        silhouette_avg,\n",
    "    )\n",
    "\n",
    "    sample_silhouette_values = silhouette_samples(x, cluster_labels)\n",
    "\n",
    "    y_lower = 10\n",
    "    for i in range(n_clusters):\n",
    "        ith_cluster_silhouette_values = sample_silhouette_values[cluster_labels == i]\n",
    "        ith_cluster_silhouette_values.sort()\n",
    "\n",
    "        size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "        y_upper = y_lower + size_cluster_i\n",
    "\n",
    "        color = cm.nipy_spectral(float(i) / n_clusters)\n",
    "        ax1.fill_betweenx(\n",
    "            np.arange(y_lower, y_upper),\n",
    "            0,\n",
    "            ith_cluster_silhouette_values,\n",
    "            facecolor=color,\n",
    "            edgecolor=color,\n",
    "            alpha=0.7,\n",
    "        )\n",
    "\n",
    "        ax1.text(-1, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "        y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "    ax1.set_title(\"The silhouette plot for the various clusters.\")\n",
    "    ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "    ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "    ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "    ax1.set_yticks([])  # Clear the y-axis labels / ticks\n",
    "    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hierarchical clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, silhouette_samples\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "\n",
    "# Range of cluster numbers to test\n",
    "n_clusters_range = range(2, 11)  # Testing for 2 to 5 clusters\n",
    "\n",
    "# List to store inertia values\n",
    "inertia = []\n",
    "\n",
    "# Calculate inertia for each number of clusters\n",
    "for n_clusters in n_clusters_range:\n",
    "    agglomerative = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "    agglomerative_labels = agglomerative.fit_predict(x)\n",
    "\n",
    "    # Calculate inertia\n",
    "    inertia_value = 0\n",
    "    for i in range(n_clusters):\n",
    "        cluster_points = x[agglomerative_labels == i]\n",
    "        if len(cluster_points) > 0:\n",
    "            inertia_value += np.sum(pairwise_distances(cluster_points, metric='euclidean').sum(axis=1))\n",
    "    \n",
    "    inertia.append(inertia_value)\n",
    "\n",
    "# Plotting the Elbow Method\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(n_clusters_range, inertia, marker='o')\n",
    "plt.title('Elbow Method for Optimal k')\n",
    "plt.xlabel('Number of clusters (k)')\n",
    "plt.ylabel('Inertia (Sum of Squared Distances)')\n",
    "plt.xticks(n_clusters_range)\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Silhouette Analysis\n",
    "for n_clusters in n_clusters_range:\n",
    "    fig, ax1 = plt.subplots(1, 1, figsize=(8, 6))\n",
    "\n",
    "    agglomerative = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "    agglomerative_labels = agglomerative.fit_predict(x)\n",
    "\n",
    "    # Only proceed if there are more than 1 cluster\n",
    "    if len(set(agglomerative_labels)) > 1:\n",
    "        silhouette_avg = silhouette_score(x, agglomerative_labels)\n",
    "        sample_silhouette_values = silhouette_samples(x, agglomerative_labels)\n",
    "\n",
    "        print(f\"For n_clusters = {n_clusters}, The average silhouette_score is : {silhouette_avg}\")\n",
    "\n",
    "        # Initialize the silhouette plot\n",
    "        y_lower = 10\n",
    "        for i in range(len(set(agglomerative_labels))):\n",
    "            ith_cluster_silhouette_values = sample_silhouette_values[agglomerative_labels == i]\n",
    "            ith_cluster_silhouette_values.sort()\n",
    "\n",
    "            size_cluster_i = ith_cluster_silhouette_values.shape[0]\n",
    "            y_upper = y_lower + size_cluster_i\n",
    "\n",
    "            color = cm.nipy_spectral(float(i) / len(set(agglomerative_labels)))\n",
    "            ax1.fill_betweenx(np.arange(y_lower, y_upper),\n",
    "                              0,\n",
    "                              ith_cluster_silhouette_values,\n",
    "                              facecolor=color,\n",
    "                              edgecolor=color,\n",
    "                              alpha=0.7)\n",
    "\n",
    "            ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))\n",
    "\n",
    "            y_lower = y_upper + 10  # 10 for the 0 samples\n",
    "\n",
    "        ax1.set_title(f\"The silhouette plot for n_clusters = {n_clusters}.\")\n",
    "        ax1.set_xlabel(\"The silhouette coefficient values\")\n",
    "        ax1.set_ylabel(\"Cluster label\")\n",
    "\n",
    "        ax1.axvline(x=silhouette_avg, color=\"red\", linestyle=\"--\")\n",
    "\n",
    "        ax1.set_yticks([])  # Clear the y-axis labels / ticks\n",
    "        ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])\n",
    "\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"For n_clusters = {n_clusters}, there are not enough clusters to compute silhouette score.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## conclusion from both clustering techniques (hirarchial and k-means) and by checking both the elbow method and the schilouette score it has been determined that a good no of clusters is 6 where this on silhouette test no big differences have taken place after 6 clusters and also by elpow method 6 clusters provide a good inertia  \n",
    "\n",
    "   despite the fact that more clusters gives better inertia and better silohette score yet this can also mean overfitting to data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## fraud detection by considering farthest point in each cluster as an anomaly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "from sklearn.decomposition import PCA # will be used just to be able to visualize \n",
    "\n",
    "\n",
    "n_clusters = 6\n",
    "\n",
    "# Perform hierarchical clustering\n",
    "agglomerative = AgglomerativeClustering(n_clusters=n_clusters)\n",
    "final_labels = agglomerative.fit_predict(x)\n",
    "\n",
    "# Calculate silhouette score\n",
    "if len(set(final_labels)) > 1:\n",
    "    silhouette_avg = silhouette_score(x, final_labels)\n",
    "    print(f\"The average silhouette_score for {n_clusters} clusters is : {silhouette_avg}\")\n",
    "else:\n",
    "    print(f\"Not enough clusters to compute silhouette score.\")\n",
    "\n",
    "# Create a DataFrame to hold the points and their corresponding cluster labels\n",
    "clustered_data = x.copy()\n",
    "clustered_data['Cluster'] = final_labels\n",
    "\n",
    "# Identify the farthest 5% of points in each cluster for potential fraud detection\n",
    "fraud_indices = []\n",
    "fraud_counts = np.zeros(n_clusters)  # Array to hold the count of frauds in each cluster\n",
    "\n",
    "for i in range(n_clusters):\n",
    "    # Get points in the current cluster\n",
    "    cluster_points = x[final_labels == i]\n",
    "    \n",
    "    # Calculate distances from the centroid of the cluster\n",
    "    centroid = np.mean(cluster_points, axis=0)\n",
    "    distances = np.linalg.norm(cluster_points - centroid, axis=1)\n",
    "    \n",
    "    # Determine the threshold for the farthest 5%\n",
    "    threshold_index = int(len(distances) * 0.95)  # 95% index\n",
    "    farthest_points_indices = np.argsort(distances)[threshold_index:]  # Get the farthest 5%\n",
    "    \n",
    "    # Store the indices of these points\n",
    "    current_fraud_indices = np.where(final_labels == i)[0][farthest_points_indices]\n",
    "    fraud_indices.extend(current_fraud_indices)\n",
    "    \n",
    "    # Count the number of frauds in the current cluster\n",
    "    fraud_counts[i] = len(current_fraud_indices)\n",
    "\n",
    "# Convert to a NumPy array for easier manipulation\n",
    "fraud_indices = np.array(fraud_indices)\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=2)\n",
    "x_pca = pca.fit_transform(x)\n",
    "\n",
    "# Visualization of each cluster with potential frauds\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot all points with their clusters\n",
    "for i in range(n_clusters):\n",
    "    cluster_points = x_pca[final_labels == i]\n",
    "    plt.scatter(cluster_points[:, 0], cluster_points[:, 1], label=f'Cluster {i}', alpha=0.5)\n",
    "\n",
    "# Plot potential frauds\n",
    "fraud_points = x_pca[fraud_indices]\n",
    "plt.scatter(fraud_points[:, 0], fraud_points[:, 1], color='red', marker='x', s=100, label='Potential Fraud')\n",
    "\n",
    "plt.title('Clusters with Potential Frauds (PCA Reduced)')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()\n",
    "\n",
    "# Print the indices of potential frauds\n",
    "print(\"\\nIndices of potential frauds:\", fraud_indices)  \n",
    "print(\"Number of frauds in each cluster:\", fraud_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
